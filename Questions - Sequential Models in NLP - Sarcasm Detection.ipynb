{"cells":[{"cell_type":"markdown","metadata":{"id":"pp68FAQf9aMN"},"source":["# Sarcasm Detection      (Total marks: 40)"]},{"cell_type":"markdown","metadata":{"id":"vAk6BRUh8CqL"},"source":["### Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v8-PQsV0DrAZ"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"z6pXf7A78E2H"},"source":["### Drop `article_link` from dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-WUNHq5zEV0n"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"D0h6IOxU8OdH"},"source":["### Get length of each headline and add a column for that"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MLpiBRDmEV2l"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"SMF-wjJ2aMwm"},"source":["### Initialize parameter values\n","- Set values for max_features, maxlen, & embedding_size\n","- max_features: Number of words to take from tokenizer(most frequent words)\n","- maxlen: Maximum length of each sentence to be limited to 25\n","- embedding_size: size of embedding vector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jPw9gAN_EV6m"},"outputs":[],"source":["max_features = 10000\n","maxlen = 25\n","embedding_size = 200"]},{"cell_type":"markdown","metadata":{"id":"9abSe-bM8fn9"},"source":["### Apply `tensorflow.keras` Tokenizer and get indices for words\n","- Initialize Tokenizer object with number of words as 10000\n","- Fit the tokenizer object on headline column\n","- Convert the text to sequence\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y8g4l0KfF3eh"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"xeZpwPO4bOkZ"},"source":["### Pad sequence\n","- Pad each example with a maximum length\n","- Convert target column into numpy array"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qV0K70E5c9Xl"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"WJLyKg-98rH_"},"source":["### Vocab mapping\n","- There is no word for 0th index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vCNgtnQqdbZn"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"VRiNX58Rb3oJ"},"source":["### Set number of words\n","- Since the above 0th index doesn't have a word, add 1 to the length of the vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dfwq6ou8ck2f"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"bUF1TuQa8ux0"},"source":["### Load Glove Word Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vq5AIfRtMeZh"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"prHSzdQUcZhm"},"source":["### Create embedding matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"elZ-T5aFGZmZ"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"u7IbWuEX82Ra"},"source":["### Define model\n","- Hint: Use Sequential model instance and then add Embedding layer, Bidirectional(LSTM) layer, flatten it, then dense and dropout layers as required.\n","In the end add a final dense layer with sigmoid activation for binary classification."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4tv168Gmc3PY"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"xoI7_8Y1cqTj"},"source":["### Compile the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-jJiPHeNoJ3U"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"7s4nmqcecw3a"},"source":["### Fit the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NN789zNnJ5PL"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}